# Binary-Classification
In this assignment, I focused on binary classification using three prominent machine learning algorithms: K-Nearest Neighbors (KNN), Naive Bayes, and Logistic Regression. These algorithms were implemented in Python using the scikit-learn library. The dataset, a well-known benchmark for classification tasks, was split into training and testing sets. I employed 10-fold cross-validation on the training set to optimize hyperparameters and evaluated model performance using accuracy, precision, recall, and F1-score metrics.

KNN is a non-parametric algorithm that classifies data based on the k-nearest neighbors. Naive Bayes is a probabilistic algorithm relying on Bayes' theorem, assuming feature independence. Logistic Regression is a linear model for binary classification, estimating class probabilities using a sigmoid function. Hyperparameters were fine-tuned to enhance model performance.

Results showed KNN outperformed Naive Bayes and Logistic Regression, achieving an accuracy of 98.9%. Naive Bayes achieved 94.4% accuracy, while Logistic Regression reached 96.7%. Evaluating each model's precision, recall, and F1-score provided insights into their strengths and weaknesses. KNN demonstrated robust performance with minimal misclassifications, while Naive Bayes showed higher false negatives, and Logistic Regression had balanced misclassifications.

Further analysis compared the performance of the models, with KNN emerging as the top performer. I discussed the significance of proper data preparation, hyperparameter tuning, and evaluation in model development. Additionally, the assignment highlighted future research avenues, including exploring advanced classification methods and optimizing data preparation techniques.

Overall, this assignment provided practical experience in implementing and evaluating classification algorithms, underscoring the importance of methodological considerations in model development and analysis.







